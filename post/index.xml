<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Articles on Kolors</title>
    <link>https://Kwai-Kolors.github.io/post/</link>
    <description>Recent content in Articles on Kolors</description>
    <generator>Hugo</generator>
    <language>en</language>
    <lastBuildDate>Fri, 21 Jun 2024 11:00:59 -0400</lastBuildDate>
    <atom:link href="https://Kwai-Kolors.github.io/post/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>ã€ŠKolors: Unet is enough for diffusionã€‹</title>
      <link>https://Kwai-Kolors.github.io/post/post-2/</link>
      <pubDate>Fri, 21 Jun 2024 11:00:59 -0400</pubDate>
      <guid>https://Kwai-Kolors.github.io/post/post-2/</guid>
      <description>Table of Contents ğŸ“– Model Introduction ğŸ“Š Evaluation Performance ğŸ¥‡ğŸ¥‡ğŸ”¥ğŸ”¥ ğŸ¥ Visualization ğŸ› ï¸ Quick Start ğŸ“œ License and Citation ğŸ“– Model Introduction We have open-sourced the Kolors large model, which is a large-scale text-to-image generation model based on latent diffusion. The current open-source model has 2.7 billion parameters. Kolors is trained on billions of text-image pairs and demonstrates significant advantages in visual quality, complex semantic understanding, and even text generation (Chinese and English characters) compared to both open-source and closed-source models.</description>
    </item>
    <item>
      <title>ã€ŠLearning Multi-dimensional Human Preference for Text-to-Image Generationã€‹ï¼ŒCVPR-2024</title>
      <link>https://Kwai-Kolors.github.io/post/post-1/</link>
      <pubDate>Thu, 20 Jun 2024 10:58:08 -0400</pubDate>
      <guid>https://Kwai-Kolors.github.io/post/post-1/</guid>
      <description>Authors Sixian Zhang, Bohan Wang, Junqiang Wu*, Yan Liâ€¡, Tingting Gao, Di Zhang, Zhongyuan Wang&#xA;Links Paper Code arXiv Abstract Current metrics for text-to-image models typically rely on statistical metrics which inadequately represent the real preference of humans. Although recent works attempt to learn these preferences via human annotated images, they reduce the rich tapestry of human preference to a single overall score. However, the preference results vary when humans evaluate images with different aspects.</description>
    </item>
  </channel>
</rss>
