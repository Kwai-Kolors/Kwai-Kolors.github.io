<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Researches on Kolors</title>
    <link>https://Kwai-Kolors.github.io/post/</link>
    <description>Recent content in Researches on Kolors</description>
    <generator>Hugo</generator>
    <language>en</language>
    <lastBuildDate>Fri, 21 Jun 2024 11:00:59 -0400</lastBuildDate>
    <atom:link href="https://Kwai-Kolors.github.io/post/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Kolors: Effective Training of Diffusion Model for Photorealistic Text-to-Image Synthesis (arXiv-2024)</title>
      <link>https://Kwai-Kolors.github.io/post/post-2/</link>
      <pubDate>Fri, 21 Jun 2024 11:00:59 -0400</pubDate>
      <guid>https://Kwai-Kolors.github.io/post/post-2/</guid>
      <description>Kolors is a large-scale text-to-image generation model based on latent diffusion, developed by the Kuaishou Kolors team. Trained on billions of text-image pairs, Kolors exhibits significant advantages over both open-source and proprietary models in visual quality, complex semantic accuracy, and text rendering for both Chinese and English characters. Furthermore, Kolors supports both Chinese and English inputs, demonstrating strong performance in understanding and generating Chinese-specific content. For more details, please refer to this &lt;strong&gt;&lt;a href=&#34;https://kolors.kuaishou.com/&#34;&gt;technical report&lt;/a&gt;&lt;/strong&gt;.</description>
    </item>
    <item>
      <title>Learning Multi-dimensional Human Preference for Text-to-Image Generation (CVPR-2024)</title>
      <link>https://Kwai-Kolors.github.io/post/post-1/</link>
      <pubDate>Thu, 20 Jun 2024 10:58:08 -0400</pubDate>
      <guid>https://Kwai-Kolors.github.io/post/post-1/</guid>
      <description>To learn the multi-dimensional human preferences, we propose the Multi-dimensional Preference Score (MPS), the first multi-dimensional preference scoring model for the evaluation of text-to-image models. The MPS introduces the preference condition module upon CLIP model to learn these diverse preferences. It is trained based on our Multi-dimensional Human Preference (MHP) Dataset, which comprises 918,315 human preference choices across 4 dimensions on 607,541 images. The images are generated by a wide range of latest text-to-image models. The MPS outperforms existing scoring methods across 3 datasets in 4 dimensions, enabling it a promising metric for evaluating and improving text-to-image generation.</description>
    </item>
  </channel>
</rss>
