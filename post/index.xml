<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Researches on Kolors</title>
    <link>https://Kwai-Kolors.github.io/post/</link>
    <description>Recent content in Researches on Kolors</description>
    <generator>Hugo</generator>
    <language>en</language>
    <lastBuildDate>Fri, 21 Jun 2024 11:00:59 -0400</lastBuildDate>
    <atom:link href="https://Kwai-Kolors.github.io/post/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Kolors: Unet is enough for diffusion (arXiv-2024)</title>
      <link>https://Kwai-Kolors.github.io/post/post-2/</link>
      <pubDate>Fri, 21 Jun 2024 11:00:59 -0400</pubDate>
      <guid>https://Kwai-Kolors.github.io/post/post-2/</guid>
      <description>We have open-sourced the &lt;strong&gt;Kolors&lt;/strong&gt; large model, which is a large-scale text-to-image generation model based on latent diffusion. The current open-source model has 2.7 billion parameters. Kolors is trained on billions of text-image pairs and demonstrates significant advantages in visual quality, complex semantic understanding, and even text generation (Chinese and English characters) compared to both open-source and closed-source models. Additionally, Kolors supports both Chinese and English, making it more competitive in understanding Chinese-specific content. The model link is provided in the table below; please click to learn more.</description>
    </item>
    <item>
      <title>Learning Multi-dimensional Human Preference for Text-to-Image Generation (CVPR-2024)</title>
      <link>https://Kwai-Kolors.github.io/post/post-1/</link>
      <pubDate>Thu, 20 Jun 2024 10:58:08 -0400</pubDate>
      <guid>https://Kwai-Kolors.github.io/post/post-1/</guid>
      <description>To learn the multi-dimensional human preferences, we propose the Multi-dimensional Preference Score (MPS), the first multi-dimensional preference scoring model for the evaluation of text-to-image models. The MPS introduces the preference condition module upon CLIP model to learn these diverse preferences. It is trained based on our Multi-dimensional Human Preference (MHP) Dataset, which comprises 918,315 human preference choices across 4 dimensions on 607,541 images. The images are generated by a wide range of latest text-to-image models. The MPS outperforms existing scoring methods across 3 datasets in 4 dimensions, enabling it a promising metric for evaluating and improving text-to-image generation.</description>
    </item>
  </channel>
</rss>
