<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
    
    <title>《Learning Multi-dimensional Human Preference for Text-to-Image Generation》，CVPR-2024 | Kolors</title>
    <meta name="viewport" content="width=device-width,minimum-scale=1">
    <meta name="description" content="Kolors Team">
    <meta name="generator" content="Hugo 0.127.0">
    
    
    
    
      <meta name="robots" content="noindex, nofollow">
    

    
<link rel="stylesheet" href="/ananke/css/main.min.css" >



  
    <link rel="stylesheet" href="/css/custom.css">
  

    
    
    
      
<link rel="shortcut icon" href="/favicon/favicon.ico" type="image/x-icon" />


    

    
    
    <meta property="og:url" content="https://Kwai-Kolors.github.io/post/post-1/">
  <meta property="og:site_name" content="Kolors">
  <meta property="og:title" content="《Learning Multi-dimensional Human Preference for Text-to-Image Generation》，CVPR-2024">
  <meta property="og:description" content="Kolors Team">
  <meta property="og:locale" content="en">
  <meta property="og:type" content="article">
    <meta property="article:section" content="post">
    <meta property="article:published_time" content="2024-06-20T10:58:08-04:00">
    <meta property="article:modified_time" content="2024-06-20T10:58:08-04:00">
    <meta property="article:tag" content="Text-to-Image">

  <meta itemprop="name" content="《Learning Multi-dimensional Human Preference for Text-to-Image Generation》，CVPR-2024">
  <meta itemprop="description" content="Kolors Team">
  <meta itemprop="datePublished" content="2024-06-20T10:58:08-04:00">
  <meta itemprop="dateModified" content="2024-06-20T10:58:08-04:00">
  <meta itemprop="wordCount" content="747">
  <meta itemprop="keywords" content="Text-to-Image">
  <meta name="twitter:card" content="summary">
  <meta name="twitter:title" content="《Learning Multi-dimensional Human Preference for Text-to-Image Generation》，CVPR-2024">
  <meta name="twitter:description" content="Kolors Team">

	<link rel="shortcut icon" href="/favicon/favicon.ico" />
<link rel="apple-touch-icon" sizes="180x180" href="/favicon/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="/favicon/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="/favicon/favicon-16x16.png">
<link rel="manifest" href="/favicon/site.webmanifest">
  </head>

  <body class="ma0 avenir bg-near-white">

    
   
  

  
  <header class="cover bg-top" style="background-image: url('https://Kwai-Kolors.github.io/images/post-1.png'); height: 80vh; background-size: cover; background-position: center;">
    <div class="bg-black-60" style="height: 80vh;">
      <nav class="pv3 ph3 ph4-ns" role="navigation">
  <div class="flex-l justify-between items-center center">
    <a href="/" class="f3 fw2 hover-white no-underline white-90 dib">
      
        <img src="/images/logo_thumbnail.png" class="w100 mw5-ns" alt="Kolors" />
      
    </a>
    <div class="flex-l items-center">
      

      
        <ul class="pl0 mr3">
          
          <li class="list f5 f4-ns fw4 dib pr3">
            <a class="hover-white no-underline white-90" href="/about/" title="About Us page">
              About Us
            </a>
          </li>
          
          <li class="list f5 f4-ns fw4 dib pr3">
            <a class="hover-white no-underline white-90" href="/post/" title="Articles page">
              Articles
            </a>
          </li>
          
          <li class="list f5 f4-ns fw4 dib pr3">
            <a class="hover-white no-underline white-90" href="/contact/" title="Contact page">
              Contact
            </a>
          </li>
          
        </ul>
      
      
<div class="ananke-socials">
  
    <a href="https://github.com/Kwai-Kolors" target="_blank" class="github ananke-social-link link-transition stackoverflow link dib z-999 pt3 pt0-l mr1" title="GitHub link" rel="noopener" aria-label="follow on GitHub——Opens in a new window">
      
        <span class="icon"><svg style="enable-background:new 0 0 512 512;" version="1.1" viewBox="0 0 512 512"  xml:space="preserve" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" >
  <path d="M256,32C132.3,32,32,134.8,32,261.7c0,101.5,64.2,187.5,153.2,217.9c11.2,2.1,15.3-5,15.3-11.1   c0-5.5-0.2-19.9-0.3-39.1c-62.3,13.9-75.5-30.8-75.5-30.8c-10.2-26.5-24.9-33.6-24.9-33.6c-20.3-14.3,1.5-14,1.5-14   c22.5,1.6,34.3,23.7,34.3,23.7c20,35.1,52.4,25,65.2,19.1c2-14.8,7.8-25,14.2-30.7c-49.7-5.8-102-25.5-102-113.5   c0-25.1,8.7-45.6,23-61.6c-2.3-5.8-10-29.2,2.2-60.8c0,0,18.8-6.2,61.6,23.5c17.9-5.1,37-7.6,56.1-7.7c19,0.1,38.2,2.6,56.1,7.7   c42.8-29.7,61.5-23.5,61.5-23.5c12.2,31.6,4.5,55,2.2,60.8c14.3,16.1,23,36.6,23,61.6c0,88.2-52.4,107.6-102.3,113.3   c8,7.1,15.2,21.1,15.2,42.5c0,30.7-0.3,55.5-0.3,63c0,6.1,4,13.3,15.4,11C415.9,449.1,480,363.1,480,261.7   C480,134.8,379.7,32,256,32z"/>
</svg>
</span>
      
<span class="new-window"><svg  height="8px"  style="enable-background:new 0 0 1000 1000;" version="1.1" viewBox="0 0 1000 1000"  xml:space="preserve" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" >
<path d="M598 128h298v298h-86v-152l-418 418-60-60 418-418h-152v-86zM810 810v-298h86v298c0 46-40 86-86 86h-596c-48 0-86-40-86-86v-596c0-46 38-86 86-86h298v86h-298v596h596z" style="fill-rule:evenodd;clip-rule:evenodd;"/>
</svg>
</span></a>
  
</div>

    </div>
  </div>
</nav>

      <div class="tc-l pv6 ph3 ph4-ns"  style="height: 100%;">
        
          <h1 class="f2 f1-l fw2 white-90 mb0 lh-title">《Learning Multi-dimensional Human Preference for Text-to-Image Generation》，CVPR-2024</h1>
          
            <h2 class="fw1 f5 f3-l white-80 measure-wide-l center lh-copy mt3 mb4">
              Kolors Team
            </h2>
          
        
      </div>
    </div>
  </header>



    <main class="pb7" role="main">
      
  
  <article class="flex-l flex-wrap justify-between mw8 center ph3">
    <header class="mt4 w-100">
      <aside class="instapaper_ignoref b helvetica tracked">
          
        ARTICLES
      </aside>
      










  <div id="sharing" class="mt3 ananke-socials">
    
  </div>


      <h1 class="f1 athelas mt3 mb1">《Learning Multi-dimensional Human Preference for Text-to-Image Generation》，CVPR-2024</h1>
      
      
      
      <time class="f6 mv4 dib tracked" datetime="2024-06-20T10:58:08-04:00">June 20, 2024</time>
      

      
      
    </header>
    <div class="nested-copy-line-height lh-copy serif f4 nested-links mid-gray pr4-l w-two-thirds-l"><h2 id="authors">Authors</h2>
<p>Sixian Zhang<sup><em></sup>, Bohan Wang<sup></em></sup>, Junqiang Wu<sup>*</sup>, Yan Li<sup>‡</sup>, Tingting Gao, Di Zhang, Zhongyuan Wang</p>
<!-- <small><sup>*</sup>Equal contribution. </small><br>
<small><sup>‡</sup>Corresponding author. </small> -->
<h2 id="links">Links</h2>
<ul>
<li><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Zhang_Learning_Multi-Dimensional_Human_Preference_for_Text-to-Image_Generation_CVPR_2024_paper.pdf">Paper</a></li>
<li><a href="https://github.com/wangbohan97/MPS_model">Code</a></li>
<li><a href="https://arxiv.org/abs/2405.14705">arXiv</a></li>
</ul>
<h2 id="abstract">Abstract</h2>
<p>Current metrics for text-to-image models typically rely on statistical metrics which inadequately represent the real preference of humans. Although recent works attempt to learn these preferences via human annotated images, they reduce the rich tapestry of human preference to a single overall score. However, the preference results vary when humans evaluate images with different aspects. Therefore, to learn the multi-dimensional human preferences, we propose the Multi-dimensional Preference Score (MPS), the first multi-dimensional preference scoring model for the evaluation of text-to-image models. The MPS introduces the preference condition module upon CLIP model to learn these diverse preferences. It is trained based on our Multi-dimensional Human Preference (MHP) Dataset, which comprises 918,315 human preference choices across 4 dimensions (i.e., aesthetics, semantic alignment, detail quality and overall assessment) on 607,541 images. The images are generated by a wide range of latest text-to-image models. The MPS outperforms existing scoring methods across 3 datasets in 4 dimensions, enabling it a promising metric for evaluating and improving text-to-image generation. The model and dataset will be made publicly available to facilitate future research.</p>
<h2 id="multi-dimensional-human-preference-mhp-dataset">Multi-dimensional Human Preference (MHP) Dataset</h2>
<p>To learn the multi-dimensional human preferences, we propose the Multi-dimensional Human Preference (MHP) dataset. Compared to prior efforts, the MHP dataset offers significant enhancements in prompts collection, image generation, and preference annotation.</p>
<ol>
<li>For the prompt collection, based on the categories schema of Parti, we annotate the collected prompts into 7 category labels (e.g., characters, scenes, objects, animals, etc.). For the underrepresented tail categories, we employ Large Language Models (LLMs) (e.g., GPT-4) to generate additional prompts. This process results in a balanced prompt collection across various categories, which is used for later image generation.</li>
<li>For image generation, we not only utilize existing open-source Diffusion models and their variants, but also employ GANs and auto-regressive models to generate images. Consequently, we generate a dataset of 607,541 images, which are further used to create 918,315 pairwise comparisons of images for preference annotation.</li>
<li>For the annotation of human preferences, contrary to the single annotation of existing work, we consider a broader range of dimensions for human preferences and employ human annotators to label each image pair across four dimensions, including aesthetics, detail quality, semantic alignment, and overall score.</li>
</ol>
<h2 id="comparisons-of-text-to-image-models-quality-databases">Comparisons of text-to-image models quality databases</h2>
<table>
<thead>
<tr>
<th>Dataset</th>
<th>Prompt Collection Source</th>
<th>Annotation</th>
<th>Image Generation Source</th>
<th>Number</th>
<th>Preference Annotation Rating</th>
<th>Dimension</th>
</tr>
</thead>
<tbody>
<tr>
<td><a href="https://poloclub.github.io/diffusiondb/">DiffusionDB</a></td>
<td>DiffusionDB</td>
<td>×</td>
<td>Diffusion(1)</td>
<td>1,819,808</td>
<td>0</td>
<td>None</td>
</tr>
<tr>
<td><a href="https://github.com/lcysyzxdxc/AGIQA-1k-Database">AGIQA-1K</a></td>
<td>DiffusionDB</td>
<td>×</td>
<td>Diffusion(2)</td>
<td>1,080</td>
<td>23,760</td>
<td>Overall</td>
</tr>
<tr>
<td><a href="https://stability.ai/research/pick-a-pic">PickScore</a></td>
<td>Web Application</td>
<td>×</td>
<td>Diffusion(3)</td>
<td>583,747</td>
<td>583,747</td>
<td>Overall</td>
</tr>
<tr>
<td><a href="https://github.com/THUDM/ImageReward">ImageReward</a></td>
<td>DiffusionDB</td>
<td>×</td>
<td>Auto Regressive; Diffusion(6)</td>
<td>136,892</td>
<td>410,676</td>
<td>Overall</td>
</tr>
<tr>
<td><a href="https://tgxs002.github.io/align_sd_web/">HPS</a></td>
<td>DiffusionDB</td>
<td>×</td>
<td>Diffusion(1)</td>
<td>98,807</td>
<td>98,807</td>
<td>Overall</td>
</tr>
<tr>
<td><a href="https://github.com/tgxs002/HPSv2">HPS v2</a></td>
<td>DiffusionDB, COCO</td>
<td>✓</td>
<td>GAN; Auto Regressive; Diffusion, COCO(9)</td>
<td>430,060</td>
<td>798,090</td>
<td>Overall</td>
</tr>
<tr>
<td><a href="https://github.com/lcysyzxdxc/AGIQA-3k-Database">AGIQA-3K</a></td>
<td>DiffusionDB</td>
<td>×</td>
<td>GAN; Auto Regressive; Diffusion(6)</td>
<td>2,982</td>
<td>125,244</td>
<td>Overall; Alignment</td>
</tr>
<tr>
<td>MHP (Ours)</td>
<td>DiffusionDB, PromptHero, KOLORS, GPT4</td>
<td>✓</td>
<td>GAN; Auto Regressive; Diffusion(9)</td>
<td>607,541</td>
<td>918,315</td>
<td>Aesthetics, Detail, Alignment, Overall</td>
</tr>
</tbody>
</table>
<h2 id="multi-dimensional-preference-score-mps">Multi-dimensional Preference Score (MPS)</h2>
<p>To learn human preferences, we propose the Multi-dimensional Preference Score (MPS), a unified model capable of predicting scores under various preference conditions.</p>
<ol>
<li>A certain preference is denoted by a series of descriptive words. For instance, the &lsquo;aesthetic&rsquo; condition is decomposed into words such as &rsquo;light&rsquo;, &lsquo;color&rsquo;, and &lsquo;clarity&rsquo; to describe the attributes of this condition.</li>
<li>These attribute words are used to compute similarities with the prompt, resulting in a similarity matrix that reflects the correspondence between words in the prompt and the specified condition.</li>
<li>Features from images and text are extracted using a pre-trained vision-language model. Subsequently, two modalities are fused through a multimodal cross-attention layer.</li>
<li>The similarity matrix serves as a mask merged into the cross-attention layer, which ensures that the text only related to the condition is attended to by the visual modality. Then the fused features are used to predict the preference scores.</li>
</ol>
<p><img alt="MPS Framework" src="/images/post-1/framework_page-0001.jpg"></p>
<h2 id="visualization-results">Visualization Results</h2>
<p>The visualization results indicate that our HPS attends to different regions of prompts and images depending on the specific preference condition. This is attributed to the condition mask, which allows only those words in the prompt related to the preference condition to be observed by the image. The condition mask ensures that the model predicts the preference with different inputs, and the model only needs to calculate the similarity between patches in the image and the retained partial prompt to determine the final score. Therefore, the selective focus enabled by the condition mask allows utilizing a unified model to predict multinational preferences effectively, even if some preferences have weak correlations with others.</p>
<p><img alt="Sample 1" src="/images/post-1/example_1.png">
<img alt="Sample 2" src="/images/post-1/example_2.png">
<img alt="Sample 3" src="/images/post-1/example_3.png">
<img alt="Sample 4" src="/images/post-1/example_4.png"></p>
<ul class="pa0">
  
   <li class="list di">
     <a href="/tags/text-to-image" class="link f5 grow no-underline br-pill ba ph3 pv2 mb2 dib black sans-serif">Text-to-Image</a>
   </li>
  
</ul>
<div class="mt6 instapaper_ignoref">
      
      
      </div>
    </div>

    <aside class="w-30-l mt6-l">




</aside>

  </article>

    </main>
    <footer class="bg-black bottom-0 w-100 pa3" role="contentinfo">
  <div class="flex justify-between">
  <a class="f4 fw4 hover-white no-underline white-70 dn dib-ns pv2 ph3" href="https://Kwai-Kolors.github.io/" >
    &copy;  Kolors 2024 
  </a>
    <div>
<div class="ananke-socials">
  
    <a href="https://github.com/Kwai-Kolors" target="_blank" class="github ananke-social-link link-transition stackoverflow link dib z-999 pt3 pt0-l mr1" title="GitHub link" rel="noopener" aria-label="follow on GitHub——Opens in a new window">
      
        <span class="icon"><svg style="enable-background:new 0 0 512 512;" version="1.1" viewBox="0 0 512 512"  xml:space="preserve" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" >
  <path d="M256,32C132.3,32,32,134.8,32,261.7c0,101.5,64.2,187.5,153.2,217.9c11.2,2.1,15.3-5,15.3-11.1   c0-5.5-0.2-19.9-0.3-39.1c-62.3,13.9-75.5-30.8-75.5-30.8c-10.2-26.5-24.9-33.6-24.9-33.6c-20.3-14.3,1.5-14,1.5-14   c22.5,1.6,34.3,23.7,34.3,23.7c20,35.1,52.4,25,65.2,19.1c2-14.8,7.8-25,14.2-30.7c-49.7-5.8-102-25.5-102-113.5   c0-25.1,8.7-45.6,23-61.6c-2.3-5.8-10-29.2,2.2-60.8c0,0,18.8-6.2,61.6,23.5c17.9-5.1,37-7.6,56.1-7.7c19,0.1,38.2,2.6,56.1,7.7   c42.8-29.7,61.5-23.5,61.5-23.5c12.2,31.6,4.5,55,2.2,60.8c14.3,16.1,23,36.6,23,61.6c0,88.2-52.4,107.6-102.3,113.3   c8,7.1,15.2,21.1,15.2,42.5c0,30.7-0.3,55.5-0.3,63c0,6.1,4,13.3,15.4,11C415.9,449.1,480,363.1,480,261.7   C480,134.8,379.7,32,256,32z"/>
</svg>
</span>
      
<span class="new-window"><svg  height="8px"  style="enable-background:new 0 0 1000 1000;" version="1.1" viewBox="0 0 1000 1000"  xml:space="preserve" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" >
<path d="M598 128h298v298h-86v-152l-418 418-60-60 418-418h-152v-86zM810 810v-298h86v298c0 46-40 86-86 86h-596c-48 0-86-40-86-86v-596c0-46 38-86 86-86h298v86h-298v596h596z" style="fill-rule:evenodd;clip-rule:evenodd;"/>
</svg>
</span></a>
  
</div>
</div>
  </div>
</footer>

  </body>
</html>
