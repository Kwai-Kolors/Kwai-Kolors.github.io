---
date: 2024-06-20T10:58:08-04:00
description: "Kolors Team"
featured_image: "/images/post-1-1.png"
summaried_image: "/images/post-1.png"
tags: ["Text-to-Image"]
title: "Learning Multi-dimensional Human Preference for Text-to-Image Generation (CVPR-2024)"
summary: "To learn the multi-dimensional human preferences, we propose the Multi-dimensional Preference Score (MPS), the first multi-dimensional preference scoring model for the evaluation of text-to-image models. The MPS introduces the preference condition module upon CLIP model to learn these diverse preferences. It is trained based on our Multi-dimensional Human Preference (MHP) Dataset, which comprises 918,315 human preference choices across 4 dimensions on 607,541 images. The images are generated by a wide range of latest text-to-image models. The MPS outperforms existing scoring methods across 3 datasets in 4 dimensions, enabling it a promising metric for evaluating and improving text-to-image generation."
---
## Links
- [Paper](https://openaccess.thecvf.com/content/CVPR2024/papers/Zhang_Learning_Multi-Dimensional_Human_Preference_for_Text-to-Image_Generation_CVPR_2024_paper.pdf)
- [Code](https://github.com/wangbohan97/MPS_model)
- [arXiv](https://arxiv.org/abs/2405.14705)

## Abstract
Current metrics for text-to-image models typically rely on statistical metrics which inadequately represent the real preference of humans. Although recent works attempt to learn these preferences via human annotated images, they reduce the rich tapestry of human preference to a single overall score. However, the preference results vary when humans evaluate images with different aspects. Therefore, to learn the multi-dimensional human preferences, we propose the Multi-dimensional Preference Score (MPS), the first multi-dimensional preference scoring model for the evaluation of text-to-image models. The MPS introduces the preference condition module upon CLIP model to learn these diverse preferences. It is trained based on our Multi-dimensional Human Preference (MHP) Dataset, which comprises 918,315 human preference choices across 4 dimensions (i.e., aesthetics, semantic alignment, detail quality and overall assessment) on 607,541 images. The images are generated by a wide range of latest text-to-image models. The MPS outperforms existing scoring methods across 3 datasets in 4 dimensions, enabling it a promising metric for evaluating and improving text-to-image generation. The model and dataset will be made publicly available to facilitate future research.

## Multi-dimensional Human Preference (MHP) Dataset
To learn the multi-dimensional human preferences, we propose the Multi-dimensional Human Preference (MHP) dataset. Compared to prior efforts, the MHP dataset offers significant enhancements in prompts collection, image generation, and preference annotation.

1. For the prompt collection, based on the categories schema of Parti, we annotate the collected prompts into 7 category labels (e.g., characters, scenes, objects, animals, etc.). For the underrepresented tail categories, we employ Large Language Models (LLMs) (e.g., GPT-4) to generate additional prompts. This process results in a balanced prompt collection across various categories, which is used for later image generation.
2. For image generation, we not only utilize existing open-source Diffusion models and their variants, but also employ GANs and auto-regressive models to generate images. Consequently, we generate a dataset of 607,541 images, which are further used to create 918,315 pairwise comparisons of images for preference annotation.
3. For the annotation of human preferences, contrary to the single annotation of existing work, we consider a broader range of dimensions for human preferences and employ human annotators to label each image pair across four dimensions, including aesthetics, detail quality, semantic alignment, and overall score.

## Comparisons of text-to-image models quality databases
| Dataset | Prompt Collection Source | Annotation | Image Generation Source | Number | Preference Annotation Rating | Dimension |
| --- | --- | --- | --- | --- | --- | --- |
| [DiffusionDB](https://poloclub.github.io/diffusiondb/) | DiffusionDB | × | Diffusion(1) | 1,819,808 | 0 | None |
| [AGIQA-1K](https://github.com/lcysyzxdxc/AGIQA-1k-Database) | DiffusionDB | × | Diffusion(2) | 1,080 | 23,760 | Overall |
| [PickScore](https://stability.ai/research/pick-a-pic) | Web Application | × | Diffusion(3) | 583,747 | 583,747 | Overall |
| [ImageReward](https://github.com/THUDM/ImageReward) | DiffusionDB | × | Auto Regressive; Diffusion(6) | 136,892 | 410,676 | Overall |
| [HPS](https://tgxs002.github.io/align_sd_web/) | DiffusionDB | × | Diffusion(1) | 98,807 | 98,807 | Overall |
| [HPS v2](https://github.com/tgxs002/HPSv2) | DiffusionDB, COCO | ✓ | GAN; Auto Regressive; Diffusion, COCO(9) | 430,060 | 798,090 | Overall |
| [AGIQA-3K](https://github.com/lcysyzxdxc/AGIQA-3k-Database) | DiffusionDB | × | GAN; Auto Regressive; Diffusion(6) | 2,982 | 125,244 | Overall; Alignment |
| MHP (Ours) | DiffusionDB, PromptHero, KOLORS, GPT4 | ✓ | GAN; Auto Regressive; Diffusion(9) | 607,541 | 918,315 | Aesthetics, Detail, Alignment, Overall |

## Multi-dimensional Preference Score (MPS)
To learn human preferences, we propose the Multi-dimensional Preference Score (MPS), a unified model capable of predicting scores under various preference conditions.

1. A certain preference is denoted by a series of descriptive words. For instance, the 'aesthetic' condition is decomposed into words such as 'light', 'color', and 'clarity' to describe the attributes of this condition.
2. These attribute words are used to compute similarities with the prompt, resulting in a similarity matrix that reflects the correspondence between words in the prompt and the specified condition.
3. Features from images and text are extracted using a pre-trained vision-language model. Subsequently, two modalities are fused through a multimodal cross-attention layer.
4. The similarity matrix serves as a mask merged into the cross-attention layer, which ensures that the text only related to the condition is attended to by the visual modality. Then the fused features are used to predict the preference scores.

<div style="text-align: center;">
    <img src="/images/post-1/framework_page-0001.jpg"  style="width:70%;height:auto;">
</div>

## Visualization Results
The visualization results indicate that our HPS attends to different regions of prompts and images depending on the specific preference condition. This is attributed to the condition mask, which allows only those words in the prompt related to the preference condition to be observed by the image. The condition mask ensures that the model predicts the preference with different inputs, and the model only needs to calculate the similarity between patches in the image and the retained partial prompt to determine the final score. Therefore, the selective focus enabled by the condition mask allows utilizing a unified model to predict multinational preferences effectively, even if some preferences have weak correlations with others.


<div style="text-align: center;">
    <img src="/images/post-1/example_1.png"  style="width:70%;height:auto;">
</div>
<div style="text-align: center;">
    <img src="/images/post-1/example_2.png"  style="width:70%;height:auto;">
</div>
<div style="text-align: center;">
    <img src="/images/post-1/example_3.png"  style="width:70%;height:auto;">
</div>
<div style="text-align: center;">
    <img src="/images/post-1/example_4.png"  style="width:70%;height:auto;">
</div>